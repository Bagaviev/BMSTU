{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1 , shape = shape )\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d (x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    variable_summaries(W)\n",
    "    b = bias_variable([shape[3]])\n",
    "    variable_summaries(b)\n",
    "    return tf.nn.relu(conv2d(input, W) + b)\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int (input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    variable_summaries(W)\n",
    "    b = bias_variable([size])\n",
    "    variable_summaries(b)\n",
    "    return tf.matmul(input, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-9145c3351aa5>:41: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tmp\\data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tmp\\data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting tmp\\data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp\\data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# This helper function , taken from the official TensorFlow documentation ,\n",
    "# simply adds some ops that take care of logging summaries\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "LOG_DIR = 'logs/summaries'\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None , 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "with tf.name_scope('conv_1'):\n",
    "    conv1 = conv_layer(x_image, shape = [5, 5, 1, 32])\n",
    "    conv1_pool = max_pool_2x2(conv1)\n",
    "\n",
    "with tf.name_scope('conv_2'):\n",
    "    conv2 = conv_layer(conv1_pool, shape = [5, 5, 32, 64])\n",
    "    conv2_pool = max_pool_2x2(conv2)\n",
    "    conv2_flat = tf.reshape(conv2_pool, [-1, 7*7*64])\n",
    "    \n",
    "with tf.name_scope('full_1'):\n",
    "    full_1 = tf.nn.relu(full_layer(conv2_flat , 1024))\n",
    "    \n",
    "with tf.name_scope('dropout'):\n",
    "    full1_drop = tf.nn.dropout(full_1, keep_prob = keep_prob)\n",
    "    \n",
    "with tf.name_scope('activations'):\n",
    "    y_conv = full_layer(full1_drop, 10)\n",
    "    variable_summaries(y_conv)\n",
    "    mnist = input_data.read_data_sets('tmp\\data', one_hot = True)\n",
    "    \n",
    "with tf.name_scope('cross'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = y_conv, labels = y_))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "# # SGD\n",
    "# AdaGrad\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate = 1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.5693137645721436, step 0, training accuracy 0.1599999964237213\n",
      "time 21.375672340393066, step 100, training accuracy 0.30000001192092896\n",
      "time 43.35503268241882, step 200, training accuracy 0.2800000011920929\n",
      "time 66.34480905532837, step 300, training accuracy 0.23999999463558197\n",
      "time 87.40769600868225, step 400, training accuracy 0.4399999976158142\n",
      "time 109.66989302635193, step 500, training accuracy 0.4000000059604645\n",
      "time 132.51177525520325, step 600, training accuracy 0.6600000262260437\n",
      "time 152.6121952533722, step 700, training accuracy 0.5600000023841858\n",
      "time 173.50917768478394, step 800, training accuracy 0.47999998927116394\n",
      "time 193.3457715511322, step 900, training accuracy 0.6800000071525574\n",
      "time 214.18678426742554, step 1000, training accuracy 0.800000011920929\n",
      "time 234.2851254940033, step 1100, training accuracy 0.6600000262260437\n",
      "time 254.05074000358582, step 1200, training accuracy 0.5600000023841858\n",
      "time 273.7664008140564, step 1300, training accuracy 0.6399999856948853\n",
      "time 293.1562542915344, step 1400, training accuracy 0.699999988079071\n",
      "time 313.01283049583435, step 1500, training accuracy 0.6200000047683716\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep = 7, keep_checkpoint_every_n_hours = 1)\n",
    "DIR = 'models'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train', graph = tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test', graph = tf.get_default_graph())\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range (5000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            summary, train_accuracy = sess.run([merged, accuracy],\n",
    "            feed_dict = {x: batch[0], y_:batch[1], keep_prob : 1.0})\n",
    "            print (\"time {}, step {}, training accuracy {}\".format(time.time () - start_time, i, train_accuracy))\n",
    "            # Add to summaries\n",
    "            train_writer.add_summary(summary, i)\n",
    "        if i%1000 == 0:\n",
    "            saver.save(sess, os.path.join(DIR, 'model_ckpt'), global_step = i)\n",
    "        sess.run(train_step, feed_dict = {x:batch[0], y_:batch[1], keep_prob : 0.5})\n",
    "\n",
    "    X = mnist.test.images.reshape(10, 1000, 784)\n",
    "    Y = mnist.test.labels.reshape(10 , 1000 , 10)\n",
    "    summary , test_accuracy = sess.run([merged , accuracy], feed_dict = {x: X[1], y_: Y[1],\n",
    "    keep_prob: 1.0}) \n",
    "    \n",
    "    test_writer.add_summary(summary, i)\n",
    "    print (\"test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Humanity restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, os.path.join(DIR, 'model_ckpt-4000'))\n",
    "    ans = sess.run(accuracy, feed_dict = {x: X[0], y_: Y[0], keep_prob: 1.0}) \n",
    "    print(\"Accuracy: {:.4}%\".format(ans*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
